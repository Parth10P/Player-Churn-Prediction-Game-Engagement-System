{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå≤ Random Forest Model ‚Äî Player Churn Prediction\n",
                "\n",
                "Train and evaluate a Random Forest classifier for predicting player churn.  \n",
                "**Issue #6** requirements: F1 > 0.75, confusion matrix, feature importance, save model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score, precision_score, recall_score,\n",
                "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
                ")\n",
                "\n",
                "# Project root\n",
                "BASE_DIR = os.path.dirname(os.getcwd())   # notebooks -> project root\n",
                "print(f\"Project root: {BASE_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load & Preprocess Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(os.path.join(BASE_DIR, \"data\", \"online_gaming_behavior_dataset.csv\"))\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable\n",
                "df[\"Churned\"] = (df[\"EngagementLevel\"] == \"Low\").astype(int)\n",
                "print(f\"Churn rate: {df['Churned'].mean():.2%}\")\n",
                "\n",
                "# Encode categoricals\n",
                "categorical_cols = [\"Gender\", \"Location\", \"GameGenre\", \"GameDifficulty\"]\n",
                "label_encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    df[col] = le.fit_transform(df[col])\n",
                "    label_encoders[col] = le\n",
                "\n",
                "# Feature engineering\n",
                "df[\"EngagementScore\"]    = df[\"SessionsPerWeek\"] * df[\"AvgSessionDurationMinutes\"]\n",
                "df[\"ProgressionRate\"]    = df[\"PlayerLevel\"] / (df[\"PlayTimeHours\"] + 1)\n",
                "df[\"PurchaseFrequency\"]  = df[\"InGamePurchases\"] / (df[\"PlayTimeHours\"] + 1)\n",
                "df[\"IsInactive\"]         = (df[\"SessionsPerWeek\"] <= 2).astype(int)\n",
                "df[\"SessionConsistency\"] = (df[\"SessionsPerWeek\"] > 3).astype(int)\n",
                "\n",
                "print(f\"Features after engineering: {df.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train / test split\n",
                "X = df.drop(columns=[\"PlayerID\", \"EngagementLevel\", \"Churned\"])\n",
                "y = df[\"Churned\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Scale\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
                "X_test_scaled  = pd.DataFrame(scaler.transform(X_test),      columns=X_test.columns,  index=X_test.index)\n",
                "\n",
                "feature_names = list(X_train.columns)\n",
                "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
                "print(f\"Features ({len(feature_names)}): {feature_names}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train Random Forest (n_estimators=100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "model.fit(X_train_scaled, y_train)\n",
                "print(\"‚úÖ Model trained\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test_scaled)\n",
                "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
                "\n",
                "acc     = accuracy_score(y_test, y_pred)\n",
                "prec    = precision_score(y_test, y_pred)\n",
                "rec     = recall_score(y_test, y_pred)\n",
                "f1      = f1_score(y_test, y_pred)\n",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "\n",
                "print(f\"Accuracy:  {acc:.4f}\")\n",
                "print(f\"Precision: {prec:.4f}\")\n",
                "print(f\"Recall:    {rec:.4f}\")\n",
                "print(f\"F1 Score:  {f1:.4f}  {'‚úÖ > 0.75' if f1 > 0.75 else '‚ùå < 0.75'}\")\n",
                "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
                "print()\n",
                "print(classification_report(y_test, y_pred, target_names=[\"Active\", \"Churned\"]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
                "            xticklabels=[\"Active\", \"Churned\"],\n",
                "            yticklabels=[\"Active\", \"Churned\"])\n",
                "plt.title(\"Random Forest ‚Äî Confusion Matrix\")\n",
                "plt.xlabel(\"Predicted\")\n",
                "plt.ylabel(\"Actual\")\n",
                "plt.tight_layout()\n",
                "\n",
                "os.makedirs(os.path.join(BASE_DIR, \"notebooks\", \"plots\"), exist_ok=True)\n",
                "cm_path = os.path.join(BASE_DIR, \"notebooks\", \"plots\", \"confusion_matrix_rf.png\")\n",
                "plt.savefig(cm_path)\n",
                "print(f\"Saved to {cm_path}\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "importances = model.feature_importances_\n",
                "indices = np.argsort(importances)[::-1]\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(\n",
                "    [feature_names[i] for i in indices],\n",
                "    importances[indices],\n",
                "    color=sns.color_palette(\"viridis\", len(feature_names)),\n",
                ")\n",
                "plt.xlabel(\"Importance\")\n",
                "plt.title(\"Random Forest ‚Äî Feature Importance\")\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "\n",
                "fi_path = os.path.join(BASE_DIR, \"notebooks\", \"plots\", \"feature_importance_rf.png\")\n",
                "plt.savefig(fi_path)\n",
                "print(f\"Saved to {fi_path}\")\n",
                "plt.show()\n",
                "\n",
                "# Print table\n",
                "print(\"\\nFeature Importance Ranking:\")\n",
                "for rank, i in enumerate(indices, 1):\n",
                "    print(f\"  {rank:2d}. {feature_names[i]:30s} {importances[i]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Results & Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_dir = os.path.join(BASE_DIR, \"models\")\n",
                "os.makedirs(models_dir, exist_ok=True)\n",
                "\n",
                "# Save metrics\n",
                "results_path = os.path.join(models_dir, \"rf_results.txt\")\n",
                "with open(results_path, \"w\") as f:\n",
                "    f.write(\"Random Forest Evaluation Metrics\\n\")\n",
                "    f.write(\"================================\\n\")\n",
                "    f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
                "    f.write(f\"Precision: {prec:.4f}\\n\")\n",
                "    f.write(f\"Recall: {rec:.4f}\\n\")\n",
                "    f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
                "    f.write(f\"ROC-AUC: {roc_auc:.4f}\\n\")\n",
                "print(f\"‚úÖ Results saved to {results_path}\")\n",
                "\n",
                "# Save model\n",
                "model_path = os.path.join(models_dir, \"rf_model.pkl\")\n",
                "joblib.dump(model, model_path)\n",
                "print(f\"‚úÖ Model saved to {model_path}\")\n",
                "\n",
                "print(f\"\\nüéâ All done! F1 = {f1:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}